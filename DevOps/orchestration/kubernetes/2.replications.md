# Replications

You can create, supervise, and manage pods manually. But in real-world use cases, you want your deployments to stay up and running automatically and remain healthy without any manual intervention. To do this, you almost never create pods directly. Instead, you create other types of resources, such as ReplicationControllers or Deployments, which then create and manage the actual pods.
When you create unmanaged pods, a cluster node is selected to run the pod and then its containers are run on that node.Kubernetes then monitors those containers and automatically restarts them if they fail. But if the whole node fails, the pods on the node are lost and will not be replaced with new ones, unless those pods are managed by the ReplicationControllers.

**liveness probes** - Kubernetes can check if a container is still alive through liveness probes. You can specify a liveness probe for each container in the pod’s specification. Kubernetes will periodically execute the probe and restart the container if the probe fails.

NOTE:- Kubernetes also supports readiness probes.

Kubernetes can probe a container using one of the three mechanisms:
1. An HTTP GET probe performs an HTTP GET request on the container’s IP address, a port and path you specify. If the probe receives a response, and the response code doesn’t represent an error (in other words, if the HTTP response code is 2xx or 3xx), the probe is considered successful. If the server returns an error response code or if it doesn’t respond at all, the probe is considered a failure and the container will be restarted as a result.
2. A TCP Socket probe tries to open a TCP connection to the specified port of the container. If the connection is established successfully, the probe is successful.
Otherwise, the container is restarted.
3. An Exec probe executes an arbitrary command inside the container and checks the command’s exit status code. If the status code is 0, the probe is successful.
All other codes are considered failures.

## REPLICATIONCONTROLLER

A ReplicationController is a Kubernetes resource that ensures its pods are always kept running. If the pod disappears for any reason, such as in the event of a node disappearing from the cluster or because the pod was evicted from the node, the ReplicationController notices the missing pod and creates a replacement pod.

It makes sure there’s always exactly one instance of your pod running. Generally, ReplicationControllers are used to replicate pods (that is, create multiple copies of a pod) and keep them running.

If you dodn’t specify how many pod replicas you want, so the ReplicationController creates a single one. If your pod were to disappear for any reason, the
ReplicationController would create a new pod to replace the missing one.

One of the main benefits of using Kubernetes is the simplicity with which you can scale your deployments. Let’s see how easy it is to scale up the number of pods. You’ll
increase the number of running instances to three.

```bash
kubectl get replicationcontroller
```

To scale up the number of replicas of your pod, you need to change the desired replica count on the ReplicationController like this:

```bash
kubectl scale rc kubia --replicas=3
```

A ReplicationController constantly monitors the list of running pods and makes sure the actual number of pods of a “type” always matches the desired number. If too few
such pods are running, it creates new replicas from a pod template. If too many such pods are running, it removes the excess replicas.

A ReplicationController has three essential parts:-

1. A label selector, which determines what pods are in the ReplicationController’s scope
2. A replica count, which specifies the desired number of pods that should be running
3. A pod template, which is used when creating new pod replicas

A ReplicationController’s replica count, the label selector, and even the pod template can all be modified at any time, but only changes to the replica count affect existing pods.

Changes to the label selector and the pod template have no effect on existing pods.Changing the label selector makes the existing pods fall out of the scope of the
ReplicationController, so the controller stops caring about them. ReplicationControllers also don’t care about the actual “contents” of its pods (the container images,
environment variables, and other things) after they create the pod. The template therefore only affects new pods created by this ReplicationController.

A ReplicationController, although an incredibly simple concept, provides or enables the following powerful features:

1. It makes sure a pod (or multiple pod replicas) is always running by starting a new pod when an existing one goes missing.
2. When a cluster node fails, it creates replacement replicas for all the pods that were running on the failed node (those that were under the ReplicationController’s control).
3. It enables easy horizontal scaling of pods—both manual and automatic

NOTE- A pod instance is never relocated to another node. Instead, the ReplicationController creates a completely new pod instance that has no relation to the instance it’s replacing.

- `Creating a ReplicationController` - you create a ReplicationController by posting a JSON or YAML descriptor to the Kubernetes API server.

```yaml
apiVersion: v1 #Descriptor conforms to version v1 of Kubernetes API.
kind: ReplicationConroller # defines a ReplicationController(RC).
metadata:
   name: {ControllerName} #The name of the ReplicationController.
spec:
   replicas: 3 #Desired number of pod instances
   selector:
      app: {Pods} # Pod selector determining what pods RC is operating on
   template:
      metadata:
         labels:
            app: {label}
      spec:
         containers:
            - name: {container-name}
              image: {AppImage} #Container image to create the container from
              ports:
               - containerPort: 8080 #The port the app is listening on
```

To create the ReplicationController, use the kubectl create command, which you already know:

```bash
kubectl create -f kubia-rc.yaml
#replicationcontroller "kubia" created
```

A ReplicationController’s pod template can be modified at any time. Changing the pod template is like replacing a cookie cutter with another one. It will only affect the cookies you cut out afterward and will have no effect on the ones you’ve already cut

```bash
kubectl edit rc kubia
```

This will open the ReplicationController’s YAML definition in your default text editor.

Initially, ReplicationControllers were the only Kubernetes component for replicating pods and rescheduling them when nodes failed. Later, a similar resource called a
ReplicaSet was introduced. It’s a new generation of ReplicationController and replaces it completely (ReplicationControllers will eventually be deprecated).

## REPLICASETS

You usually won’t create them directly, but instead have them created automatically when you create the higher-level Deployment resource
A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive pod selectors. Whereas a ReplicationController’s label selector only allows matching pods that include a certain label, a ReplicaSet’s selector also allows matching pods that lack a certain label or pods that include a certain label key, regardless of its value.
Also, for example, a single ReplicationController can’t match pods with the label env=production and those with the label env=devel at the same time. It can only match
either pods with the env=production label or pods with the env=devel label. But a single ReplicaSet can match both sets of pods and treat them as a single group.
Similarly, a ReplicationController can’t match pods based merely on the presence of a label key, regardless of its value, whereas a ReplicaSet can. For example, a ReplicaSet can match all pods that include a label with the key env, whatever its actual value is (you can think of it as env=*).

- `Creating a ReplicaSet` - you create a ReplicationSet by posting a JSON or YAML descriptor to the Kubernetes API server.

```yaml
apiVersion: apps/v1beta2 #ReplicaSets aren't part of v1,but belongs to the apps API group and version v1beta2
kind: ReplicaSet # defines a ReplicationController(RC).
metadata:
   name: {ControllerName} #The name of the ReplicaSet.
spec:
   replicas: 3 #Desired number of pod instances
   selector:
      matchLabels:
         app: {Pods} # Pod selector determining what pods RC is operating on
   template:
      metadata:
         labels:
            app: {label}
      spec:
         containers:
            - name: {container-name}
              image: {AppImage} #Container image to create the container from
              ports:
               - containerPort: 8080 #The port the app is listening on
```

## DAEMONSET

A DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods should only run on a subset of all the nodes. This is done by specifying the node-Selector property in the pod template, which is part of the DaemonSet definition (similar to the pod template in a ReplicaSet or ReplicationController).
You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3.
A node selector in a DaemonSet is similar—it defines the nodes the DaemonSet must deploy its pods to.

```yaml
apiVersion: apps/v1beta2 #DaemonSet belongs to the apps API group and version v1beta2
kind: DaemonSet
metadata:
   name: {ControllerName}
spec:
   selector:
      matchLabels:
         app: {Pods} # Pod selector determining what pods RC is operating on
   template:
      metadata:
         labels:
            app: {label}
      spec:
         nodeSelector:
            disk: ssd
         containers:
            - name: {container-name}
              image: {AppImage} #Container image to create the container from.
```

