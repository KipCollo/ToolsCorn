# PODS

Kubernetes doesn’t deal with individual containers directly. Instead, it uses the concept of multiple co-located containers. This group of containers is called a Pod.Containers are not managed individually; instead, they are part of a larger object called a Pod. A Pod consists of one or more containers which share an IP address, access to storage and namespace. Typically, one container in a Pod runs an application, while other containers support the primary application.

A pod is a group of one or more tightly related containers that will always run together on the same worker node and in the same Linux namespace(s).
Each pod is like a separate logical machine with its own IP, hostname, processes, and so on, running a single application. The application can be a single process, running in a single container, or it can be a main application process and additional supporting processes, each running in its own container. All the containers in a pod will appear to be running on the same logical machine, whereas containers in other pods, even if they’re running on the same worker node, will appear to be running on a different one.

Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.
A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific "logical host": it contains one or more application containers which are relatively tightly coupled. In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.

As well as application containers, a Pod can contain init containers that run during Pod startup. You can also inject ephemeral containers for debugging a running Pod.

A pod is a co-located group of containers and represents the basic building block in Kubernetes.Instead of deploying containers individually,you always deploy and operate on a pod of containers. We’re not implying that a pod always includes more than one container—it’s common for pods to contain only a single container. The key thing about pods is that when a pod does contain multiple containers, all of them are always run on a single worker node—it never spans multiple worker nodes.

It contains only a single container, but generally a pod can contain as many containers as you want,storage resources,unique IP and options to govern how container(s) run.The pod has its own unique private IP address and hostname.Pods are ephemeral and disposable,they never self-heal and not restarted by scheduler.It includes the states:- Pending,Running,Succeeded,Failed,CrashLoopBackOff.

Everything else either manages, exposes, or is used by pods.


The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container. Within a Pod's context, the individual applications may have further sub-isolations applied.

A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.

Pods in a Kubernetes cluster are used in two main ways:

1. Pods that run a single container. The "one-container-per-Pod" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly.
2. Pods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit.

Grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this pattern only in specific instances in which your containers are tightly coupled.
You don't need to run multiple containers to provide replication (for resilience or capacity); if you need multiple replicas, see Workload management.

The whole point of Kubernetes is to orchestrate the life cycle of a container. We do not interact with particular containers. Instead, the smallest unit we can work with is a Pod. Some would say a pod of whales or peas-in-a-pod. Due to shared resources, the design of a Pod typically follows a one-process-per-container architecture.

Containers in a Pod are started in parallel by default. As a result, there is no way to determine which container becomes available first inside a Pod. initContainers can be used to ensure some containers are ready before others in a pod. To support a single process running in a container, you may need logging, a proxy, or special adapter. These tasks are often handled by other containers in the same Pod.

There is only one IP address per Pod with most network plugins. HPE Labs created a plugin which allows more than one IP per pod. As a result, if there is more than one container, they must share the IP. To communicate with each other, they can use IPC, the loopback interface, or a shared filesystem.

While Pods are often deployed with one application container in each, a common reason to have multiple containers in a Pod is for logging. You may find the term sidecar for a container dedicated to performing a helper task, like handling logs and responding to requests, as the primary application container may not have this ability.

Note:- You need to install a container runtime into each node in the cluster so that Pods can run there.
The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container. Within a Pod's context, the individual applications may have further sub-isolations applied.

Other objects will be created by operators/watch-loops to ensure the specifications and current status are the same.

Pods are generally not created directly and are created using workload resources.
Usually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as Deployment or Job. If your Pods need to track state, consider the StatefulSet resource.

Each Pod is meant to run a single instance of a given application. If you want to scale your application horizontally (to provide more overall resources by running more instances), you should use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as replication. Replicated Pods are usually created and managed as a group by a workload resource and its controller.

Pods natively provide two kinds of shared resources for their constituent containers: networking and storage.

You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This is because Pods are designed as relatively ephemeral, disposable entities. When a Pod gets created (directly by you, or indirectly by a controller), the new Pod is scheduled to run on a Node in your cluster. The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is evicted for lack of resources, or the node fails.
Note:- Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for running container(s). A Pod persists until it is deleted.


## Why Pods

Containers are designed to run only a single process per container (unless the process itself spawns child processes). If you run multiple unrelated processes in a single container, it is your responsibility to keep all those processes running, manage their logs, and so on. For example, you’d have to include a mechanism for automatically restarting individual processes if they crash. Also, all those processes would log to the same standard output, so you’d have a hard time figuring out what process logged what.
Therefore, you need to run each process in its own container. That’s how Docker and Kubernetes are meant to be used.

Because you’re not supposed to group multiple processes into a single container, it’s obvious you need another higher-level construct that will allow you to bind containers together and manage them as a single unit. This is the reasoning behind pods.
A pod of containers allows you to run closely related processes together and provide them with (almost) the same environment as if they were all running in a single container, while keeping them somewhat isolated. This way, you get the best of both worlds. You can take advantage of all the features containers provide, while at the same time giving the processes the illusion of running together.


## FLAT INTER-POD NETWORK

All pods in a Kubernetes cluster reside in a single flat, shared, network-address space which means every pod can access every other pod at the other pod’s IP address. No NAT (Network Address Translation) gateways exist between them.When two pods send network packets between each other, they’ll each see the actual IP address of the other as the source IP in the packet.

Consequently, communication between pods is always simple. It doesn’t matter if two pods are scheduled onto a single or onto different worker nodes; in both cases the containers inside those pods can communicate with each other across the flat NATless network

Each pod gets its own IP address and is accessible from all other pods through this network established specifically for pods. This is usually achieved through an additional software-defined network layered on top of the actual network.

`Single IP per Pod` A Pod represents a group of co-located containers with some associated data volumes. All containers in a Pod share the same network namespace.
To communicate with each other, containers can use the loopback interface, write to files on a common filesystem, or via inter-process communication (IPC). As a result, co-locating applications in the same pod may have issues. There is a network plugin which will allow more than one IP address, but so far, it has only been used within HPE labs.

Support for dual-stack, IPv4 and IPv6 continues to increase with each release. For example, in a recent release kube-proxy iptables supports both stacks simultaneously.

A Pod is a group of co-located containers that share the same IP address. From a networking perspective, a Pod can be seen as a virtual machine of physical hosts. The network needs to assign IP addresses to Pods, and needs to provide traffic routes between all Pods on any nodes.

The three main networking challenges to solve in a container orchestration system are:

1. Coupled container-to-container communications (solved by the Pod concept)
2. Pod-to-Pod communications
3. External-to-Pod communications.

Kubernetes expects the network configuration to enable Pod-to-Pod communications to be available; it will not do it for you.

Pods contain shared resources(e.g volumes) for all Pod containers.
Pods has a cluster-internal IP by default.Containers inside a Pod can communicat via localhost.
Pods are assigned an IP address prior to application containers being started. The service object is used to connect Pods within the network using ClusterIP addresses, from outside of the cluster using NodePort addresses, and using a load balancer if configured with a LoadBalancer service.


## Creating PODS

Pods and other objects can be created in several ways:-

1. Using run
2. using JSON/YAML files


**Using run command**:- They can be created by using run:

```bash
kubectl run <pod_name> --image=nginx
```


**Creating pods from YAML or JSON descriptors**:- Pods and other Kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint.Defining all your Kubernetes objects from YAML files makes it possible to store them in a version control system, with all the benefits it brings.


Pod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts.

- apiVersion: v1
- kind: Pod
- metadata (ObjectMeta) - Standard object's metadata.Includes the name, namespace, labels, and other information about the pod
- spec (PodSpec) - Specification of the desired behavior of the pod.Contains the actual description of the pod’s contents, such as the pod’s containers, volumes, and other data.
- status (PodStatus) - Most recently observed status of the pod. This data may not be up to date. Populated by the system. Read-only.Contains the current information about the running pod, such as what condition the pod is in, the description and status of each container, and the pod’s internal IP and other basic info.

PodSpec - PodSpec is a description of a pod

- `Containers` - 
   1. containers ([]Container), required - Map: unique values on key name will be kept during a merge.List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated.
   2. initContainers ([]Container) - List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started. If any init container fails, the pod is considered to have failed and is handled according to its restartPolicy. The name for an init container or normal container must be unique among all containers. Init containers may not have Lifecycle actions, Readiness probes, Liveness probes, or Startup probes. The resourceRequirements of an init container are taken into account during scheduling by finding the highest request/limit for each resource type, and then using the max of that value or the sum of the normal containers. Limits are applied to init containers in a similar fashion. Init containers cannot currently be added or removed. Cannot be updated.
   3. ephemeralContainers ([]EphemeralContainer) - List of ephemeral containers run in this pod. Ephemeral containers may be run in an existing pod to perform user-initiated actions such as debugging. This list cannot be specified when creating a pod, and it cannot be modified by updating the pod spec. In order to add an ephemeral container to an existing pod, use the pod's ephemeralcontainers subresource.

Container - A single application container that you want to run within a pod.

1. name (string), required - Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated.
2. Image:-
   - image (string) - Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets.
   - imagePullPolicy (string) - Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images
   Possible enum values:
      1.  "Always" means that kubelet always attempts to pull the latest image. Container will fail If the pull fails.
      2.  "IfNotPresent" means that kubelet pulls if the image isn't present on disk. Container will fail if the image isn't present and the pull fails.
      3.  "Never" means that kubelet never pulls an image, but only uses a local image. Container will fail if the image isn't present
   - ports ([]ContainerPort) - List of ports to expose from the container. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0" address inside a container will be accessible from the network. Modifying this array with strategic merge patch may corrupt the data.
   ContainerPort represents a network port in a single container.
      1. ports.containerPort (int32), required - Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536.
      2. ports.hostIP (string) - What host IP to bind the external port to.
      3. ports.hostPort (int32) - Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this.
      4. ports.name (string) - If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have a unique name. Name for the port that can be referred to by services.
      5. ports.protocol (string) - Protocol for port. Must be UDP, TCP, or SCTP. Defaults to "TCP".
      Possible enum values:
        - "SCTP" is the SCTP protocol.
        - "TCP" is the TCP protocol.
        - "UDP" is the UDP protocol.

- `Lifecycle`
   1. restartPolicy (string) - Restart policy for all containers within the pod. One of Always, OnFailure, Never. In some contexts, only a subset of those values may be permitted. Default to Always.
   Possible enum values:
      - "Always"
      - "Never"
      - "OnFailure"


The following is an example of a Pod which consists of a container running the image nginx:1.14.2.

```yml
apiVersion: v1 #Descriptor conforms to version v1 of kubernetes API
kind: Pod
metadata:
  name: nginx #Name of pod
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
      protocol: TCP
```

Specifying ports in the pod definition is purely informational. Omitting them has no effect on whether clients can connect to the pod through the port or not. If the container is accepting connections through a port bound to the 0.0.0.0 address, other pods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But it makes sense to define the ports explicitly so that everyone using your cluster can quickly see what ports each pod exposes. Explicitly defining ports also allows you to assign a name to each port.

When preparing a manifest, you can either turn to the Kubernetes reference documentation at http://kubernetes.io/docs/api to see which attributes are supported by each API object, or you can use the kubectl explain command.

```sh
kubectl explain pods
```

Kubectl prints out the explanation of the object and lists the attributes the object can contain. You can then drill deeper to find out more about each attribute. For example, you can examine the spec attribute like this:

```sh
kubectl explain pod.spec
```

To create the pod from your YAML file, use the kubectl create command.They can be created and deleted using properly formatted JSON or YAML files:

```bash
kubectl create -f newpod.yaml
kubectl delete -f newpod.yaml
```

To create the Pod shown above, run the following command:

```bash
kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
```

**RETRIEVING THE WHOLE DEFINITION OF A RUNNING POD**:- If you already have some existing pods, you can use `kubectl get` command with `-o yaml` option to get the whole YAML definition of the pod.

```sh
kubectl get pod <pod_name> -o yaml
kubectl get pod <pod_name> -o json
```

**LIST OF PODS**:- Because you can’t list individual containers, since they’re not standalone Kubernetes objects, you can list pods instead:-

```bash
kubectl get pods
```

To see more information about the pod, you can also use the kubectl describe pod command:-

```bash
kubectl describe pods
```

**application logs**:- Containerized applications usually log to the standard output and standard error stream instead of writing their logs to files. This is to allow users to view logs of different applications in a simple, standard way.
The container runtime (Docker in your case) redirects those streams to files and allows you to get the container’s log by running

```sh
docker logs <container_id>
```

You could use ssh to log into the node where your pod is running and retrieve its logs with docker logs, but Kubernetes provides an easier way.
To see your pod’s log (more precisely, the container’s log) you run the following command on your local machine (no need to ssh anywhere):

```sh
kubectl logs <pod_name>
```

NOTE:- Container logs are automatically rotated daily and every time the log file reaches 10MB in size. The kubectl logs command only shows the log entries from the last rotation.
If your pod includes multiple containers, you have to explicitly specify the container name by including the -c <container name> option when running kubectl logs.

Note that you can only retrieve container logs of pods that are still in existence. When a pod is deleted, its logs are also deleted. To make a pod’s logs available even after the pod is deleted, you need to set up centralized, cluster-wide logging, which stores all the logs into a central store.


**FORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD**:- When you want to talk to a specific pod without going through a service (for debugging or other reasons), Kubernetes allows you to configure port forwarding to the pod. This is done through the kubectl port-forward command.

```sh
kubectl port-forward <pod_name> 8888:8080
```

The port forwarder is running and you can now connect to your pod through the local port.


## Labels

Organizing pods and all other Kubernetes objects is done through labels.
Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not only pods, but all other Kubernetes resources. 
A label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors (resources are filtered based on whether they include the label specified in the selector). A resource can have more than one label, as long as the keys of those labels are unique within that resource. You usually attach labels to resources when you create them, but you can also add additional labels or even modify the values of existing labels later without having to recreate the resource.

Specifying labels when creating a pod:-

```yml
apiVersion: v1
kind: Pod
metadata:
   name: kubia-manual-v2
   labels:
      creation_method: manual
      env: prod
```

The kubectl get pods command doesn’t list any labels by default, but you can see them by using the --show-labels switch:

```sh
kubectl get po --show-labels
```

Instead of listing all labels, if you’re only interested in certain labels, you can specify them with the -L switch and have each displayed in its own column.

```sh
kubectl get po -L creation_method,env
```

Labels can also be added to and modified on existing pods.You need to use the --overwrite option when changing existing labels.

```sh
kubectl label po kubia-manual creation_method=manual
kubectl label po kubia-manual-v2 env=debug --overwrite
```

**Label selectors** allow you to select a subset of pods tagged with certain labels and perform an operation on those pods. A label selector is a criterion, which filters resources based on whether they include a certain label with a certain value.
A label selector can select resources based on whether the resource
1. Contains (or doesn’t contain) a label with a certain key
2. Contains a label with a certain key and value
3. Contains a label with a certain key, but with a value not equal to the one you specify


To list all pods that include the env label, whatever its value is:

```sh
kubectl get po -l env
```

And those that don’t have the env label:

```sh
kubectl get po -l '!env'
```

NOTE:- Make sure to use single quotes around !env, so the bash shell doesn’t evaluate the exclamation mark.

A selector can also include multiple comma-separated criteria. Resources need to match all of them to match the selector. If, for example, you want to select only pods running the beta release of the product catalog microservice, you’d use the following selector: app=pc,rel=beta
Label selectors aren’t useful only for listing pods, but also for performing actions on a subset of all pods.

**Using labels and selectors to constrain pod scheduling**:- Certain cases exist where you’ll want to have at least a little say in where a pod should be scheduled. A good example is when your hardware infrastructure isn’t homogenous. If part of your worker nodes have spinning hard drives, whereas others have SSDs, you may want to schedule certain pods to one group of nodes and the rest to the other.
You never want to say specifically what node a pod should be scheduled to, because that would couple the application to the infrastructure, whereas the whole idea of Kubernetes is hiding the actual infrastructure from the apps that run on it. But if you want to have a say in where a pod should be scheduled, instead of specifying an exact node, you should describe the node requirements and then let Kubernetes select a node that matches those requirements. This can be done through node labels and node label selectors.

Labels can be attached to any Kubernetes object, including nodes.
Usually, when the ops team adds a new node to the cluster, they’ll categorize the node by attaching labels specifying the type of hardware the node provides or anything else that may come in handy when scheduling pods.

```sh
kubectl label node gke-kubia-85f6-node-0rrx gpu=true
```

Now you can use a label selector when listing the nodes, like you did before with pods.List only nodes that include the label gpu=true:

```sh
kubectl get nodes -l gpu=true
```

Now imagine you want to deploy a new pod that needs a GPU to perform its work.
To ask the scheduler to only choose among the nodes that provide a GPU, you’ll add a node selector to the pod’s YAML.

```yml
spec:
   nodeSelector: # nodeSelector tells Kubernetes to deploy this pod only to nodes containing the gpu=true label.
      gpu: "true"
   containers:
   - image: luksa/kubia
     name: kubia
```

Similarly, you could also schedule a pod to an exact node, because each node also has a unique label with the key kubernetes.io/hostname and value set to the actual hostname of the node. But setting the nodeSelector to a specific node by the hostname label may lead to the pod being unschedulable if the node is offline. You shouldn’t think in terms of individual nodes. Always think about logical groups of nodes that satisfy certain criteria specified through label selectors.

## Annotations

In addition to labels, pods and other objects can also contain annotations. Annotations are also key-value pairs, so in essence, they’re similar to labels, but they aren’t meant to hold identifying information. They can’t be used to group objects the way labels can. While objects can be selected through label selectors, there’s no such thing as an annotation selector.
On the other hand, annotations can hold much larger pieces of information and are primarily meant to be used by tools. Certain annotations are automatically added to objects by Kubernetes, but others are added by users manually.
Annotations are also commonly used when introducing new features to Kubernetes. Usually, alpha and beta versions of new features don’t introduce any new fields to API objects. Annotations are used instead of fields, and then once the required API changes have become clear and been agreed upon by the Kubernetes developers, new fields are introduced and the related annotations deprecated.
A great use of annotations is adding descriptions for each pod or other API object, so that everyone using the cluster can quickly look up information about each individual object. For example, an annotation used to specify the name of the person who created the object can make collaboration between everyone working on the cluster much easier.

`Looking up an object’s annotations`:- To see the annotations added automatically, you’ll need to request the full YAML of the pod or use the kubectl describe command.

```yml
apiVersion: v1
kind: pod
metadata:
   annotations:
      kubernetes.io/created-by: |
         {"kind":"SerializedReference", "apiVersion":"v1",
         "reference":{"kind":"ReplicationController", "namespace":"default", ...
```

Without going into too many details, as you can see, the kubernetes.io/created-by annotation holds JSON data about the object that created the pod. That’s not some- thing you’d want to put into a label. Labels should be short, whereas annotations can contain relatively large blobs of data (up to 256 KB in total).


NOTE:- The kubernetes.io/created-by annotations was deprecated in version 1.8 and will be removed in 1.9, so you will no longer see it in the YAML.

`Adding and modifying annotations`:- Annotations can obviously be added to pods at creation time, the same way labels can. They can also be added to or modified on existing pods later. The simplest way to add an annotation to an existing object is through the kubectl annotate command.

```sh
kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"
```

Calico Pod - Another possible view of a cluster with multiple Pods

## Namespaces

Kubernetes also groups objects into namespaces.
Kubernetes namespaces provide a scope for objects names. Instead of having all your resources in one single namespace, you can split them into multiple namespaces, which also allows you to use the same resource names multiple times (across different namespaces).

Using multiple namespaces allows you to split complex systems with numerous components into smaller distinct groups. They can also be used for separating resources in a multi-tenant environment, splitting up resources into production, development, and QA environments, or in any other way you may need. Resource names only need to be unique within a namespace. Two different namespaces can contain resources of the same name. But, while most types of resources are namespaced, a few aren’t. One of them is the Node resource, which is global and not tied to a single namespace.

Discovering other namespaces and their pods:-

```sh
kubectl get ns
```

To look at the pods that belong to the kube-system namespace, by telling kubectl to list pods in that namespace only:

```sh
kubectl get po --namespace kube-system
```

You can also use -n instead of --namespace.


It’s clear from the name of the namespace that these are resources related to the Kubernetes system itself. By having them in this separate namespace, it keeps everything nicely organized. If they were all in the default namespace, mixed in with the resources you create yourself, you’d have a hard time seeing what belongs where, and you might inadvertently delete system resources.
Namespaces enable you to separate resources that don’t belong together into non-overlapping groups. If several users or groups of users are using the same Kubernetes cluster, and they each manage their own distinct set of resources, they should each use their own namespace. This way, they don’t need to take any special care not to inadvertently modify or delete the other users’ resources and don’t need to concern themselves with name conflicts, because namespaces provide a scope for resource names.
Besides isolating resources, namespaces are also used for allowing only certain users access to particular resources and even for limiting the amount of computational resources available to individual users.


**Creating a namespace**:- A namespace is a Kubernetes resource like any other, so you can create it by posting a YAML file to the Kubernetes API server.

`CREATING A NAMESPACE FROM A YAML FILE` - First, create a custom-namespace.yaml file with the following listing’s contents.

```yml
apiVersion: v1
kind: Namespace
metadata:
   name: custom-namespace
```

Use kubectl to post the file to the Kubernetes API server:

```sh
kubectl create -f custom-namespace.yaml
```


`CREATING A NAMESPACE WITH KUBECTL CREATE NAMESPACE`:- You can also create namespaces with the dedicated kubectl create namespace command, which is quicker than writing a YAML file

```sh
kubectl create namespace custom-namespace
```

**Managing objects in other namespaces**:- To create resources in the namespace you’ve created, either add a namespace: custom-namespace entry to the metadata section, or specify the namespace when creating the resource with the kubectl create command:

```sh
kubectl create -f kubia-manual.yaml -n custom-namespace
```

When listing, describing, modifying, or deleting objects in other namespaces, you need to pass the --namespace (or -n) flag to kubectl. If you don’t specify the namespace, kubectl performs the action in the default namespace configured in the current kubectl context. The current context’s namespace and the current context itself can be changed through kubectl config commands.

To quickly switch to a different namespace, you can set up the following alias: alias kcd='kubectl config set-context $(kubectl config current-context) --namespace '. You can then switch between namespaces using kcd some-namespace.

NOTE:- Although namespaces allow you to isolate objects into distinct groups, which allows you to operate only on those belonging to the specified namespace, they don’t provide any kind of isolation of running objects.

**Stopping and removing pods**:- By deleting a pod, you’re instructing Kubernetes to terminate all the containers that are part of that pod. Kubernetes sends a SIGTERM signal to the process and waits a certain number of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down in time, the process is then killed through SIGKILL. To make sure your processes are always shut down gracefully, they need to handle the SIGTERM signal properly.

`Deleting a pod by name`:-

```sh
kubectl delete po <pod_name>
```

You can also delete more than one pod by specifying multiple, space-separated names (for example, kubectl delete po pod1 pod2).

`Deleting pods using label selectors`:- Instead of specifying each pod to delete by name, you can use label selectors.

```sh
kubectl delete po -l creation_method=manual
```

`Deleting pods by deleting the whole namespace`:- You can delete the whole namespace (the pods will be deleted along with the namespace automatically), using the following command

```sh
kubectl delete ns custom-namespace
```

`Deleting all pods in a namespace, while keeping the namespace`:- Instead of deleting the specific pod, tell Kubernetes to delete all pods in the current namespace by using the --all option:

```sh
kubectl delete po --all
```

`Deleting (almost) all resources in a namespace`:- You can delete the ReplicationController and the pods, as well as all the Services you’ve created, by deleting all resources in the current namespace with a single command:

```sh
kubectl delete all --all
```

The first all in the command specifies that you’re deleting resources of all types, and the --all option specifies that you’re deleting all resource instances instead of specifying them by name (you already used this option when you ran the previous delete command)

NOTE:- Deleting everything with the all keyword doesn’t delete absolutely everything. Certain resources (like Secrets),are preserved and need to be deleted explicitly.
NOTE:- The kubectl delete all --all command also deletes the kubernetes Service, but it should be recreated automatically in a few moments.
