# The Kubernetes network model

Service
Ingress
Ingress Controllers
Gateway API
EndpointSlices
Network Policies
DNS for Services and Pods
IPv4/IPv6 dual-stack
Topology Aware Routing
Networking on Windows
Service ClusterIP allocation
Service Internal Traffic Policy

The Kubernetes network model is built out of several pieces:

- Each pod in a cluster gets its own unique cluster-wide IP address.A pod has its own private network namespace which is shared by all of the containers within the pod. Processes running in different containers in the same pod can communicate with each other over localhost.
- The pod network (also called a cluster network) handles communication between pods. It ensures that (barring intentional network segmentation):
  1. All pods can communicate with all other pods, whether they are on the same node or on different nodes. Pods can communicate with each other directly, without the use of proxies or address translation (NAT).On Windows, this rule does not apply to host-network pods.
  2. Agents on a node (such as system daemons, or kubelet) can communicate with all pods on that node.
- The Service API lets you provide a stable (long lived) IP address or hostname for a service implemented by one or more backend pods, where the individual pods making up the service can change over time.
   1.  Kubernetes automatically manages EndpointSlice objects to provide information about the pods currently backing a Service.
   2.  A service proxy implementation monitors the set of Service and EndpointSlice objects, and programs the data plane to route service traffic to its backends, by using operating system or cloud provider APIs to intercept or rewrite packets.
- The Gateway API (or its predecessor, Ingress) allows you to make Services accessible to clients that are outside the cluster.A simpler, but less-configurable, mechanism for cluster ingress is available via the Service API's type: LoadBalancer, when using a supported Cloud Provider.
- NetworkPolicy is a built-in Kubernetes API that allows you to control traffic between pods, or between pods and the outside world.

In older container systems, there was no automatic connectivity between containers on different hosts, and so it was often necessary to explicitly create links between containers, or to map container ports to host ports to make them reachable by containers on other hosts. This is not needed in Kubernetes; Kubernetes's model is that pods can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and migration.

Only a few parts of this model are implemented by Kubernetes itself. For the other parts, Kubernetes defines the APIs, but the corresponding functionality is provided by external components, some of which are optional:

1. Pod network namespace setup is handled by system-level software implementing the Container Runtime Interface.
2. The pod network itself is managed by a pod network implementation. On Linux, most container runtimes use the Container Networking Interface (CNI) to interact with the pod network implementation, so these implementations are often called CNI plugins.
3. Kubernetes provides a default implementation of service proxying, called kube-proxy, but some pod network implementations instead use their own service proxy that is more tightly integrated with the rest of the implementation.
4. NetworkPolicy is generally also implemented by the pod network implementation. (Some simpler pod network implementations don't implement NetworkPolicy, or an administrator may choose to configure the pod network without NetworkPolicy support. In these cases, the API will still be present, but it will have no effect.)
5. There are many implementations of the Gateway API, some of which are specific to particular cloud environments, some more focused on "bare metal" environments, and others more generic


Pods need a way of finding other pods if they want to consume the services they provide. Unlike in the non-Kubernetes world, where a sysadmin would configure each client app by specifying the exact IP address or hostname of the server providing the service in the client’s configuration files, doing the same in Kubernetes wouldn’t work, because:-
1. Pods are ephemeral—They may come and go at any time, whether it’s because a pod is removed from a node to make room for other pods, because someone scaled down the number of pods, or because a cluster node has failed.
2. Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node and before it’s started —Clients thus can’t know the IP address of the server pod up front.
3. Horizontal scaling means multiple pods may provide the same service—Each of those pods has its own IP address. Clients shouldn’t care how many pods are backing the service and what their IPs are. They shouldn’t have to keep a list of all the individual IPs of pods. Instead, all those pods should be accessible through a single IP address.

To solve these problems, Kubernetes also provides another resource type—Services


## Service

A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service.Each service has an IP address and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing that service. This way, clients of a service don’t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster at any time.

To understand why you need services, you need to learn a key detail about pods. They’re ephemeral. A pod may disappear at any time—because the node it’s running on has failed, because someone deleted the pod, or because the pod was evicted from an otherwise healthy node. When any of those occurs, a missing pod is replaced with a new one by the ReplicationController, as described previously. This new pod gets a different IP address from the pod it’s replacing. This is where services come in—to solve the problem of ever-changing pod IP addresses, as well as exposing multiple pods at a single constant IP and port pair.

Services represent a static location for a group of one or more pods that all provide the same service. Requests coming to the IP and port of the service will be forwarded to the IP and port of one of the pods belonging to the service at that moment.
When a service is created, it gets a static IP, which never changes during the lifetime of the service. Instead of connecting to pods directly, clients should connect to the service through its constant IP address. The service makes sure one of the pods receives the connection, regardless of where the pod is currently running (and what its IP address is).

Service can be backed by more than one pod. Connections to the service are load-balanced across all the backing pods.
They act as a load balancer standing in front of multiple pods. When there’s only one pod, services provide a static address for the single pod. Whether a service is backed by a single pod or a group of pods,those pods come and go as they’re moved around the cluster, which means their IP addresses change, but the service is always there at the same address. This makes it easy for clients to connect to the pods, regardless of how many exist and how often they change location.

1. Internal service - ClusterIp and Headless service
2. External service- NodePort and Loadbalancer service

**Creating services**:-

- `CREATING A SERVICE THROUGH KUBECTL EXPOSE`:- The easiest way to create a service is through kubectl expose.The expose command created a Service resource with the same pod selector as the one used by the ReplicationController, thereby exposing all its pods through a single IP address and port

- `CREATING A SERVICE THROUGH A YAML DESCRIPTOR`

```yaml
apiVersion: v1
kind: Service
metadata:
   name: {service-name}
spec:
   ports:
   - port: 80 #The port this service will be available on
     targetPort: 8080 # The container port the service will forward to
   selector:
      app: {pod-lABEL-name} # All pods with the app={name} label will be part of this service
```

After posting the YAML, you can list all Service resources in your namespace and see that an internal cluster IP has been assigned to your service:

```sh
kubectl get services
```

- **ClusterIP (Default)**:- ClusterIP is the default Service type that exposes a service on an internal IP within the cluster. This makes the service accessible only within the cluster.

1. Key Role: Internal communication within the cluster.
2. Use Case: Communicating between microservices within the cluster.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-service
   namespace: default
spec:
   type: ClusterIP 
   ports:
   - protocol: TCP
     port: 80
     targetPort: 8080
   selector:
      app: my-app
```


**TESTING YOUR SERVICE FROM WITHIN THE CLUSTER**:- You can send requests to your service from within the cluster in a few ways:
1. The obvious way is to create a pod that will send the request to the service’s cluster IP and log the response. You can then examine the pod’s log to see what the service’s response was.
2. You can ssh into one of the Kubernetes nodes and use the curl command.
3. You can execute the curl command inside one of your existing pods through the kubectl exec command.

The kubectl exec command allows you to remotely run arbitrary commands inside an existing container of a pod. This comes in handy when you want to examine the contents, state, and/or environment of a container.

```sh
kubectl exec <pod_name> -- curl -s http://10.111.249.153
```

The double dash (--) in the command signals the end of command options for kubectl. Everything after the double dash is the command that should be executed inside the pod. Using the double dash isn’t necessary if the command has no arguments that start with a dash. But in your case, if you don’t use the double dash there, the -s option would be interpreted as an option for kubectl exec.

**SESSION AFFINITY**:- If you execute the same command a few more times, you should hit a different pod with every invocation, because the service proxy normally forwards each connection to a randomly selected backing pod, even if the connections are coming from the same client.
If, on the other hand, you want all requests made by a certain client to be redirected to the same pod every time, you can set the service’s sessionAffinity property to ClientIP (instead of None, which is the default), as shown in the following listing.

```yml
apiVersion: v1
kind: Service
spec:
   sessionAffinity: ClientIP
```

This makes the service proxy redirect all requests originating from the same client IP to the same pod.
Kubernetes supports only two types of service session affinity: None and ClientIP.You may be surprised it doesn’t have a cookie-based session affinity option, but you need to understand that Kubernetes services don’t operate at the HTTP level. Services deal with TCP and UDP packets and don’t care about the payload they carry. Because cookies are a construct of the HTTP protocol, services don’t know about them, which explains why session affinity cannot be based on cookies.

Services can also support multiple ports.
When creating a service with multiple ports, you must specify a name for each port.

```yaml
spec:
   ports:
       - name: http
         port: 80
         targetPort: 8080
       - name: https
         port: 443
         targetPort: 8443
```


**Discovering services**:- By creating a service, you now have a single and stable IP address and port that you can hit to access your pods. This address will remain unchanged throughout the whole lifetime of the service. Pods behind this service may come and go, their IPs may change, their number can go up or down, but they’ll always be accessible through the service’s single and constant IP address.
Kubernetes provides ways for client pods to discover a service’s IP and port.


**Service Endpoints**:- Services don’t link to pods directly. Instead, a resource sits in between—the Endpoints resource.
An Endpoints resource is a list of IP addresses and ports exposing a service. The Endpoints resource is like any other Kubernetes resource, so you can display its basic info with kubectl get.

```sh
kubectl get endpoints <svc_name>
```

Although the pod selector is defined in the service spec, it’s not used directly when redirecting incoming connections. Instead, the selector is used to build a list of IPs and ports, which is then stored in the Endpoints resource. When a client connects to a service, the service proxy selects one of those IP and port pairs and redirects the incoming connection to the server listening at that location.

Having the service’s endpoints decoupled from the service allows them to be configured and updated manually.
If you create a service without a pod selector, Kubernetes won’t even create the Endpoints resource (after all, without a selector, it can’t know which pods to include in the service). It’s up to you to create the Endpoints resource to specify the list of endpoints for the service.
To create a service with manually managed endpoints, you need to create both a Service and an Endpoints resource.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-service
spec:
   type: NodePort
   ports:
   - port: 80
```

Endpoints are a separate resource and not an attribute of a service. Because you created the service without a selector, the corresponding Endpoints resource hasn’t been created automatically, so it’s up to you to create it.

```yaml
apiVersion: v1
kind: Endpoints
metadata:
   name: my-service
subsets:
 - addresses:
      - ip: 11.11.11.11
      - ip: 22.22.22.22
   ports:
      - port: 80
```

The Endpoints object needs to have the same name as the service and contain the list of target IP addresses and ports for the service. After both the Service and the Endpoints resource are posted to the server, the service is ready to be used like any regular service with a pod selector. Containers created after the service is created will include the environment variables for the service, and all connections to its IP:port pair will be load balanced between the service’s endpoints.

- ExternalName:- ExternalName Services map a Kubernetes service to an external DNS name. This is useful for integrating external services into your Kubernetes environment.

1. Key Role: Redirects traffic to an external DNS name.
2. Use Case: Accessing services outside the Kubernetes cluster.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-external-service
spec:
   type: ExternalName
   external: external.example.com
```


**Exposing Services to External clients**:- You have a few ways to make a service accessible externally:
1. Setting the service type to NodePort—For a NodePort service, each cluster node opens a port on the node itself (hence the name) and redirects traffic received on that port to the underlying service. The service isn’t accessible only at the internal cluster IP and port, but also through a dedicated port on all nodes.
2. Setting the service type to LoadBalancer, an extension of the NodePort type—This makes the service accessible through a dedicated load balancer, provisioned from the cloud infrastructure Kubernetes is running on. The load balancer redirects traffic to the node port across all the nodes. Clients connect to the service through the load balancer’s IP.
3. Creating an Ingress resource, a radically different mechanism for exposing multiple services through a single IP address—It operates at the HTTP level (network layer 7) and can thus offer more features than layer 4 services can.

- `NodePort service`:- NodePort is a type of Service that exposes a service on a static port on each node in the cluster. This allows external traffic to access your application using the node’s IP address and the NodePort.
By creating a NodePort service, you make Kubernetes reserve a port on all its nodes (the same port number is used across all of them) and forward incoming connections to the pods that are part of the service.
This is similar to a regular service (their actual type is ClusterIP), but a NodePort service can be accessed not only through the service’s internal cluster IP, but also through any node’s IP and the reserved node port.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-service
spec:
   type: NodePort
   ports:
   - port: 80
     targetPort: 8080
     nodePort: 30007
   selector:
      app: my-app
```

You set the type to NodePort and specify the node port this service should be bound to across all cluster nodes. Specifying the port isn’t mandatory; Kubernetes will choose a random port if you omit it.


- **Headless Services**:- Headless Services don’t assign an IP address to the service.Instead, they return the IPs of the associated Pods directly.This is useful for stateful applications where direct access to each Pod is needed.

1. Key Role: Directly exposes Pod IPs without a ClusterIP.
2. Use Case: Stateful applications like databases where clients need to connect directly to individual Pods.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-headless-service
spec:
   ClusterIp: none
   ports:
   - port: 80
   selector:
      app: my-app
```

`Exposing a service through an external load balancer`:- Kubernetes clusters running on cloud providers usually support the automatic provision of a load balancer from the cloud infrastructure. All you need to do is set the service’s type to LoadBalancer instead of NodePort. The load balancer will have its own unique, publicly accessible IP address and will redirect all connections to your service. You can thus access your service through the load balancer’s IP address.
If Kubernetes is running in an environment that doesn’t support LoadBalancer services, the load balancer will not be provisioned, but the service will still behave like a NodePort service. That’s because a LoadBalancer service is an extension of a NodePort service.

A LoadBalancer Service creates an external load balancer that routes traffic to your Kubernetes service. This is typically used in cloud environments like AWS, Azure, or GCP.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-loadbalancer-service
spec:
   type: LoadBalancer
   ports:
   - port: 80
     targetPort: 8080
   selector:
      app: my-app
```

After you create the service, it takes time for the cloud infrastructure to create the load balancer and write its IP address into the Service object. Once it does that, the IP address will be listed as the external IP address of your service.


## Ingress

Ingress (noun)—The act of going in or entering; the right to enter; a means or place of entering; entryway.

Ingress:- Ingress is an API object that manages external access to services, typically HTTP. It provides a way to define rules for routing traffic based on the URL, host, or other parameters.

1. Key Role: Manages HTTP and HTTPS routing to services.
2. Use Case: Exposing multiple services under a single domain name with SSL termination.

**REASONS WHY INGRESSES ARE NEEDED**:-
1. One important reason is that each LoadBalancer service requires its own load balancer with its own public IP address, whereas an Ingress only requires one, even when providing access to dozens of services. When a client sends an HTTP request to the Ingress, the host and path in the request determine which service the request is forwarded to.
2. Ingresses operate at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity and the like, which services can’t.

Ingress Controllers:- Ingress Controllers are responsible for implementing the rules defined by an Ingress resource. Popular controllers include NGINX, Traefik, and HAProxy.

1. Key Role: Processes and enforces the routing rules specified in Ingress resources.
2. Use Case: Implementing custom routing, SSL, and load balancing.

Example Tools: NGINX Ingress Controller, Traefik, HAProxy.

To make Ingress resources work, an Ingress controller needs to be running in the cluster.Different Kubernetes environments use different implementations of the controller,but several don’t provide a default controller at all.
For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP load-balancing features to provide the Ingress functionality. Initially, Minikube didn’t provide a controller out of the box, but it now includes an add-on that can be enabled to let you try out the Ingress functionality.

Enabling the Ingress add-on in Minikube

```sh
minikube addons enable ingress
```

This should have spun up an Ingress controller as another pod.
You’ve confirmed there’s an Ingress controller running in your cluster, so you can now create an Ingress resource.


```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
   name: my-ingress
spec:
   rules: 
   - host: myapp.example.com
     http:
       paths:
         - path: /
           pathType: Prefix
           backend:
               service:
                  name: my-service
                  port: 
                     number: 80
```

This defines an Ingress with a single rule, which makes sure all HTTP requests received by the Ingress controller, in which the host myapp.example.com is requested, will be sent to the my-service service on port 80.

NOTE:- Ingress controllers on cloud providers (in GKE, for example) require the Ingress to point to a NodePort service. But that’s not a requirement of Kubernetes itself.

To access your service through http://myapp.example.com, you’ll need to make sure the domain name resolves to the IP of the Ingress controller.
To look up the IP, you need to list Ingresses:

```sh
kubectl get ingresses
```

When running on cloud providers, the address may take time to appear,because the Ingress controller provisions a load balancer behind the scenes.
The IP is shown in the ADDRESS column.

Once you know the IP, you can then either configure your DNS servers to resolve myapp.example.com to that IP or you can add the following line to /etc/hosts (or C:\windows\system32\drivers\etc\hosts on Windows):

192.168.99.100 myapp.example.com

## TargetPort, Port, NodePort

Understanding these ports ensures that your service is properly configured for both internal and external access.

1. Port: The port that the Service listens on.
2. TargetPort: The port on the Pod that the traffic is forwarded to.
3. NodePort: The port on each Node that routes traffic to the Service.

```yaml
ports:
 - port: 80 # Service port
   targetPort: 8080 # pod port
   nodePort: 30007 # NodePort (for NodePort services)
```

With every object and agent decoupled we need a flexible and scalable operator which connects resources together and will reconnect, should something die and a replacement is spawned. Each Service is a microservice handling a particular bit of traffic, such as a single NodePort or a LoadBalancer to distribute inbound requests among many Pods.

A Service also handles access policies for inbound requests, useful for resource control, as well as for security.

A service, as well as kubectl, uses a selector in order to know which objects to connect. There are two selectors currently supported:

    equality-based
    Filters by label keys and their values. Three operators can be used, such as =, ==, and !=. If multiple values or keys are used, all must be included for a match.
    set-based
    Filters according to a set of values. The operators are in, notin, and exists. For example, the use of status notin (dev, test, maint) would select resources with the key of status which did not have a value of dev, test, nor maint.

Operators

An important concept for orchestration is the use of operators. These are also known as watch-loops and controllers. They query the current state, compare it against the spec, and execute code based on how they differ. Various operators ship with Kubernetes, and you can create your own, as well. A simplified view of an operator is an agent, or Informer, and a downstream store. Using a DeltaFIFO queue, the source and downstream are compared. A loop process receives an obj or object, which is an array of deltas from the FIFO queue. As long as the delta is not of the type Deleted, the logic of the operator is used to create or modify some object until it matches the specification.

The Informer which uses the API server as a source requests the state of an object via an API call. The data is cached to minimize API server transactions. A similar agent is the SharedInformer; objects are often used by multiple other objects. It creates a shared cache of the state for multiple requests.

A Workqueue uses a key to hand out tasks to various workers. The standard Go workqueues of rate limiting, delayed, and time queue are typically used.

The endpoints, namespace, and serviceaccounts operators each manage the eponymous resources for Pods.



## Components Involved in Controlling Network Traffic

ClusterIP - A ClusterIP is used for traffic within the cluster. A NodePort first creates a ClusterIP, then associates a port of the node to that new ClusterIP. If you create a LoadBalancer service, it will first create a ClusterIP, then a NodePort, and then make an asynchronous request for an external load balancer. If one is not configured to respond, the EXTERNAL-IP will remain in a pending state for the life of the service.

Ingress Controller or Service Mesh - An Ingress Controller or a service mesh like Istio can also be used to connect traffic to a Pod.1


```sh
kubectl expose deployment <deploy_name>  --type=LoadBalancer--port=PortNumber
minikube service <service-name>
```
