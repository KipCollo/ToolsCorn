# The Kubernetes network model

The Kubernetes network model is built out of several pieces:

- Each pod in a cluster gets its own unique cluster-wide IP address.A pod has its own private network namespace which is shared by all of the containers within the pod. Processes running in different containers in the same pod can communicate with each other over localhost.
- The pod network (also called a cluster network) handles communication between pods. It ensures that (barring intentional network segmentation):
  1. All pods can communicate with all other pods, whether they are on the same node or on different nodes. Pods can communicate with each other directly, without the use of proxies or address translation (NAT).On Windows, this rule does not apply to host-network pods.
  2. Agents on a node (such as system daemons, or kubelet) can communicate with all pods on that node.
- The Service API lets you provide a stable (long lived) IP address or hostname for a service implemented by one or more backend pods, where the individual pods making up the service can change over time.
   1.  Kubernetes automatically manages EndpointSlice objects to provide information about the pods currently backing a Service.
   2.  A service proxy implementation monitors the set of Service and EndpointSlice objects, and programs the data plane to route service traffic to its backends, by using operating system or cloud provider APIs to intercept or rewrite packets.
- The Gateway API (or its predecessor, Ingress) allows you to make Services accessible to clients that are outside the cluster.A simpler, but less-configurable, mechanism for cluster ingress is available via the Service API's type: LoadBalancer, when using a supported Cloud Provider.
- NetworkPolicy is a built-in Kubernetes API that allows you to control traffic between pods, or between pods and the outside world.

In older container systems, there was no automatic connectivity between containers on different hosts, and so it was often necessary to explicitly create links between containers, or to map container ports to host ports to make them reachable by containers on other hosts. This is not needed in Kubernetes; Kubernetes's model is that pods can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and migration.

Only a few parts of this model are implemented by Kubernetes itself. For the other parts, Kubernetes defines the APIs, but the corresponding functionality is provided by external components, some of which are optional:

1. Pod network namespace setup is handled by system-level software implementing the Container Runtime Interface.
2. The pod network itself is managed by a pod network implementation. On Linux, most container runtimes use the Container Networking Interface (CNI) to interact with the pod network implementation, so these implementations are often called CNI plugins.
3. Kubernetes provides a default implementation of service proxying, called kube-proxy, but some pod network implementations instead use their own service proxy that is more tightly integrated with the rest of the implementation.
4. NetworkPolicy is generally also implemented by the pod network implementation. (Some simpler pod network implementations don't implement NetworkPolicy, or an administrator may choose to configure the pod network without NetworkPolicy support. In these cases, the API will still be present, but it will have no effect.)
5. There are many implementations of the Gateway API, some of which are specific to particular cloud environments, some more focused on "bare metal" environments, and others more generic


Service
Ingress
Ingress Controllers
Gateway API
EndpointSlices
Network Policies
DNS for Services and Pods
IPv4/IPv6 dual-stack
Topology Aware Routing
Networking on Windows
Service ClusterIP allocation
Service Internal Traffic Policy

## Service

To understand why you need services, you need to learn a key detail about pods. They’re ephemeral. A pod may disappear at any time—because the node it’s running on has failed, because someone deleted the pod, or because the pod was evicted from an otherwise healthy node. When any of those occurs, a missing pod is replaced with a new one by the ReplicationController, as described previously. This new pod gets a different IP address from the pod it’s replacing. This is where services come in—to solve the problem of ever-changing pod IP addresses, as well as exposing multiple pods at a single constant IP and port pair.

When a service is created, it gets a static IP, which never changes during the lifetime of the service. Instead of connecting to pods directly, clients should connect to the service through its constant IP address. The service makes sure one of the pods receives the connection, regardless of where the pod is currently running (and what its IP address is).

Services represent a static location for a group of one or more pods that all provide the same service. Requests coming to the IP and port of the service will be forwarded to the IP and port of one of the pods belonging to the service at that moment.

They act as a load balancer standing in front of multiple pods. When there’s only one pod, services provide a static address for the single pod. Whether a service is backed by a single pod or a group of pods,those pods come and go as they’re moved around the cluster, which means their IP addresses change, but the service is always there at the same address. This makes it easy for clients to connect to the pods, regardless of how many exist and how often they change location.

## Services: Enabling clients to discover and talk to pods

A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service. Each service has an IP address
and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing
that service. This way, clients of a service don’t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster
at any time.

Connections to the service are load-balanced across all the backing pods.

Pods need a way of finding other pods if they want to consume the services they provide. Unlike in the non-Kubernetes world, where a sysadmin would configure each client app by specifying the exact IP address or hostname of the server providing the service in the client’s configuration files, doing the same in Kubernetes wouldn’t work, because:-

1. Pods are ephemeral—They may come and go at any time, whether it’s because a pod is removed from a node to make room for other pods, because someone
scaled down the number of pods, or because a cluster node has failed.
2. Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node and before it’s started —Clients thus can’t know the IP address of the server pod
up front.
3. Horizontal scaling means multiple pods may provide the same service—Each of those pods has its own IP address. Clients shouldn’t care how many pods are backing
the service and what their IPs are. They shouldn’t have to keep a list of all the individual IPs of pods. Instead, all those pods should be accessible through a
single IP address.

To solve these problems, Kubernetes also provides another resource type—Services.

## EXPLAINING SERVICES WITH AN EXAMPLE

Let’s revisit the example where you have a frontend web server and a backend database server. There may be multiple pods that all act as the frontend, but there may
only be a single backend database pod. You need to solve two problems to make the system function:

1. External clients need to connect to the frontend pods without caring if there’s only a single web server or hundreds.
2. The frontend pods need to connect to the backend database. Because the database runs inside a pod, it may be moved around the cluster over time, causing
its IP address to change. You don’t want to reconfigure the frontend pods every time the backend database is moved.

By creating a service for the frontend pods and configuring it to be accessible from outside the cluster, you expose a single, constant IP address through which external
clients can connect to the pods. Similarly, by also creating a service for the backend pod, you create a stable address for the backend pod. The service address doesn’t change even if the pod’s IP address changes. Additionally, by creating the service, you also enable the frontend pods to easily find the backend service by its name through either environment variables or DNS.

1. Internal service - ClusterIp and Headless service
2. External service- NodePort and Loadbalancer service

## Creating services

- CREATING A SERVICE THROUGH KUBECTL EXPOSE:- The easiest way to create a service is through kubectl expose.The expose command created a Service resource with the same pod selector as the one used by the ReplicationController, thereby exposing all its pods through a single IP address and port

- CREATING A SERVICE THROUGH A YAML DESCRIPTOR

```yaml
apiVersion: v1
kind: Service
metadata:
   name: {service-name}
spec:
   ports:
   - port: 80 #The port this service will be available on
     targetPort: 8080 # The container port the service will forward to
   selector:
      app: {pod-lABEL-name} # All pods with the app={name} label will be part of this service
```

- **NodePort**:- NodePort is a type of Service that exposes a service on a static port on each node in the cluster. This allows external traffic to access your application using the node’s IP address and the NodePort.

1. Key Role: Exposes service externally on each node’s IP.
2. Use Case: Quick way to expose a service to external traffic without using a LoadBalancer.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-service
spec:
   type: NodePort
   ports:
   - port: 80
     targetPort: 8080
     nodePort: 30007
   selector:
      app: my-app
```

- **ClusterIP (Default)**:- ClusterIP is the default Service type that exposes a service on an internal IP within the cluster. This makes the service accessible only within the cluster.

1. Key Role: Internal communication within the cluster.
2. Use Case: Communicating between microservices within the cluster.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-service
   namespace: default
spec:
   type: ClusterIP 
   ports:
   - protocol: TCP
     port: 80
     targetPort: 8080
   selector:
      app: my-app
```

- **LoadBalancer**:- A LoadBalancer Service creates an external load balancer that routes traffic to your Kubernetes service. This is typically used in cloud environments like AWS, Azure, or GCP.

1. Key Role: Automatically provisions an external load balancer.
2. Use Case: Exposing services to the internet with automatic scaling and failover

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-loadbalancer-service
spec:
   type: LoadBalancer
   ports:
   - port: 80
     targetPort: 8080
   selector:
      app: my-app
```

- **Headless Services**:- Headless Services don’t assign an IP address to the service.Instead, they return the IPs of the associated Pods directly.This is useful for stateful applications where direct access to each Pod is needed.

1. Key Role: Directly exposes Pod IPs without a ClusterIP.
2. Use Case: Stateful applications like databases where clients need to connect directly to individual Pods.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-headless-service
spec:
   ClusterIp: none
   ports:
   - port: 80
   selector:
      app: my-app
```

- ExternalName:- ExternalName Services map a Kubernetes service to an external DNS name. This is useful for integrating external services into your Kubernetes environment.

1. Key Role: Redirects traffic to an external DNS name.
2. Use Case: Accessing services outside the Kubernetes cluster.

```yaml
apiVersion: v1
kind: Service
metadata:
   name: my-external-service
spec:
   type: ExternalName
   external: external.example.com
```

## Ingress

Ingress:- Ingress is an API object that manages external access to services, typically HTTP. It provides a way to define rules for routing traffic based on the URL, host, or other parameters.

1. Key Role: Manages HTTP and HTTPS routing to services.
2. Use Case: Exposing multiple services under a single domain name with SSL termination.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
   name: my-ingress
spec:
   rules: 
   - host: myapp.example.com
     http:
       paths:
         - path: /
           pathType: Prefix
           backend:
               service:
                  name: my-service
                  port: 
                     number: 80
```

- Ingress Controllers:- Ingress Controllers are responsible for implementing the rules defined by an Ingress resource. Popular controllers include
NGINX, Traefik, and HAProxy.

1. Key Role: Processes and enforces the routing rules specified in Ingress resources.
2. Use Case: Implementing custom routing, SSL, and load balancing.

Example Tools: NGINX Ingress Controller, Traefik, HAProxy.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
   name: my-ingress
spec:
   rules: 
   - host: example.com
     http:
       paths:
         - path: /
           pathType: Prefix
           backend:
               service:
                  name: my-service
                  port: 
                     number: 80
```

## TargetPort, Port, NodePort

Understanding these ports ensures that your service is properly configured for both internal and external access.

1. Port: The port that the Service listens on.
2. TargetPort: The port on the Pod that the traffic is forwarded to.
3. NodePort: The port on each Node that routes traffic to the Service.

```yaml
ports:
 - port: 80 # Service port
   targetPort: 8080 # pod port
   nodePort: 30007 # NodePort (for NodePort services)
```

With every object and agent decoupled we need a flexible and scalable operator which connects resources together and will reconnect, should something die and a replacement is spawned. Each Service is a microservice handling a particular bit of traffic, such as a single NodePort or a LoadBalancer to distribute inbound requests among many Pods.

A Service also handles access policies for inbound requests, useful for resource control, as well as for security.

A service, as well as kubectl, uses a selector in order to know which objects to connect. There are two selectors currently supported:

    equality-based
    Filters by label keys and their values. Three operators can be used, such as =, ==, and !=. If multiple values or keys are used, all must be included for a match.
    set-based
    Filters according to a set of values. The operators are in, notin, and exists. For example, the use of status notin (dev, test, maint) would select resources with the key of status which did not have a value of dev, test, nor maint.

Operators

An important concept for orchestration is the use of operators. These are also known as watch-loops and controllers. They query the current state, compare it against the spec, and execute code based on how they differ. Various operators ship with Kubernetes, and you can create your own, as well. A simplified view of an operator is an agent, or Informer, and a downstream store. Using a DeltaFIFO queue, the source and downstream are compared. A loop process receives an obj or object, which is an array of deltas from the FIFO queue. As long as the delta is not of the type Deleted, the logic of the operator is used to create or modify some object until it matches the specification.

The Informer which uses the API server as a source requests the state of an object via an API call. The data is cached to minimize API server transactions. A similar agent is the SharedInformer; objects are often used by multiple other objects. It creates a shared cache of the state for multiple requests.

A Workqueue uses a key to hand out tasks to various workers. The standard Go workqueues of rate limiting, delayed, and time queue are typically used.

The endpoints, namespace, and serviceaccounts operators each manage the eponymous resources for Pods.
