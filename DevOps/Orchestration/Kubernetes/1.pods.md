# PODS

Kubernetes doesn’t deal with individual containers directly. Instead, it uses the concept of multiple co-located containers. This group of containers is called a Pod.Containers are not managed individually; instead, they are part of a larger object called a Pod. A Pod consists of one or more containers which share an IP address, access to storage and namespace. Typically, one container in a Pod runs an application, while other containers support the primary application.

A pod is a group of one or more tightly related containers that will always run together on the same worker node and in the same Linux namespace(s). Each pod
is like a separate logical machine with its own IP, hostname, processes, and so on, running a single application. The application can be a single process, running in a
single container, or it can be a main application process and additional supporting processes, each running in its own container. All the containers in a pod will appear
to be running on the same logical machine, whereas containers in other pods, even if they’re running on the same worker node, will appear to be running on a different one.

Instead of deploying containers individually,you always deploy and operate on a pod of containers. We’re not implying that a pod always includes more than one container—it’s common for pods to contain only a single container. The key thing about pods is that when a pod does contain multiple containers, all of them are always run on a single worker node—it never spans multiple worker nodes.

It contains only a single container, but generally a pod can contain as many containers as you want,storage resources,unique IP and options to govern how container(s) run.The pod has its own unique private IP address and hostname.

Pods are ephemeral and disposable,they never self-heal and not restarted by scheduler.It includes the states:- Pending,Running,Succeeded,Failed,CrashLoopBackOff.

Because you can’t list individual containers, since they’re not standalone Kubernetes objects, you can list pods instead:-

```bash
kubectl get pods
```

To see more information about the pod, you can also use the kubectl describe pod command:-

```bash
kubectl describe pods
```

- Creating pods from YAML or JSON descriptors:- Pods and other Kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint. Also, you can use other, simpler ways of creating resources, such as the kubectl run command, but they usually only allow you to configure a limited set of properties, not all. Additionally, defining all your Kubernetes objects from YAML files makes it possible to store them in a version control system, with all the benefits it brings.

The pod definition consists of a few parts.

1. There’s the Kubernetes API version used in the YAML and the type of resource the YAML is describing.
2. Metadata includes the name, namespace, labels, and other information about the pod.
3. Spec contains the actual description of the pod’s contents, such as the pod’s containers, volumes, and other data.
4. Status contains the current information about the running pod, such as what condition the pod is in, the description and status of each container, and the
pod’s internal IP and other basic info.

The status part contains read-only runtime data that shows the state of the resource at a given moment. When creating a new pod, you never need to provide the status part.

```yaml
apiVersion: v1 #Descriptor conforms to version v1 of Kubernetes API
kind: Pod # You’re describing a pod
metadata:
  name: {PodName} #The name of the pod
spec:
  containers:
  - image: {AppImage} #Container image to create the container from
  name: {ContainerName} # Name of the container
  ports:
  - containerPort: 8080 #The port the app is listening on
  protocol: TCP
```

- SPECIFYING CONTAINER PORTS:- Specifying ports in the pod definition is purely informational. Omitting them has no effect on whether clients can connect to the pod through the port or not. If the container is accepting connections through a port bound to the 0.0.0.0 address, other pods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But it makes sense to define the ports explicitly so that everyone using your cluster can quickly see what ports each pod exposes. Explicitly defining ports also allows you to assign a name to each port, which can come in handy.

- Using kubectl create to create the pod:- To create the pod from your YAML file, use the kubectl create command:

```bash
kubectl create -f {name}.yaml
```

The kubectl create -f command is used for creating any resource (not only pods) from a YAML or JSON file.

- RETRIEVING THE WHOLE DEFINITION OF A RUNNING POD:- After creating the pod, you can ask Kubernetes for the full YAML of the pod. You’ll see it’s similar to the YAML you saw earlier. You’ll learn about the additional fields appearing in the returned definition in the next sections. Go ahead and use the following command to see the full descriptor of the pod:

```bash
kubectl get po kubia-manual -o yaml
```

If you’re more into JSON, you can also tell kubectl to return JSON instead of YAML like this (this works even if you used YAML to create the pod):

```bash
kubectl get po kubia-manual -o json
```

- Viewing application logs:- The application logs to the process’s standard output. Containerized applications usually log to the standard output and standard error stream instead of writing their logs to files. This is to allow users to view logs of different applications in a simple, standard way.

The container runtime (Docker in your case) redirects those streams to files and allows you to get the container’s log by running

```bash
docker logs <container id>
```

You could use ssh to log into the node where your pod is running and retrieve its logs with docker logs, but Kubernetes provides an easier way

- RETRIEVING A POD’S LOG WITH KUBECTL LOGS:- To see your pod’s log (more precisely, the container’s log) you run the following command on your local machine (no need to ssh anywhere):

```bash
kubectl logs <pod-name>
```

Retrieving logs of an application running in Kubernetes is incredibly simple if the pod only contains a single container.

NOTE Container logs are automatically rotated daily and every time the log file reaches 10MB in size. The kubectl logs command only shows the log entries
from the last rotation.

- SPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD:- If your pod includes multiple containers, you have to explicitly specify the container
name by including the -c *container name* option when running kubectl logs.

```bash
kubectl logs <pod-name> -c <container-name>
```

Note that you can only retrieve container logs of pods that are still in existence. When a pod is deleted, its logs are also deleted. To make a pod’s logs available even after the pod is deleted, you need to set up centralized, cluster-wide logging, which stores all the logs into a central store.

- FORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD:- When you want to talk to a specific pod without going through a service (for debugging or other reasons),Kubernetes allows you to configure port forwarding to the pod. This is done through the kubectl port-forward command. The following command will forward your machine’s local port 8888 to port 8080 of your pod:

```bash
kubectl port-forward kubia-manual 8888:8080
... Forwarding from 127.0.0.1:8888 -> 8080
... Forwarding from [::1]:8888 -> 8080
```

The port forwarder is running and you can now connect to your pod through the
local port.

- **Introducing labels**:-Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not only pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors (resources are filtered based on whether they include the label specified in the selector). A resource can have more than one label, as long as the keys of those labels are unique within that resource. You usually attach labels to resources when you create them, but you can also add additional labels or even modify the values of existing labels later without having to recreate the resource.

Each pod is labeled with two labels:

1. app, which specifies which app, component, or microservice the pod belongs to.E.g UI,CART
2. rel, which shows whether the application running in the pod is a stable, beta,or a canary release.

- NOTE A canary release is when you deploy a new version of an application next to the stable version, and only let a small fraction of users hit the
new version to see how it behaves before rolling it out to all users. This prevents bad releases from being exposed to too many users.

By adding these two labels, you’ve essentially organized your pods into two dimensions (horizontally by app and vertically by release)

- Specifying labels when creating a pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: {podName}
  labels:
    creation_method: manual
    env: prod
spec:
  containers:
  - image: {imageName}
  name: {containerName}
  ports:
  - containerPort: 8080
    protocol: TCP
```

The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container. Within a Pod's context, the individual applications may have further sub-isolations applied.

A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.

Pods in a Kubernetes cluster are used in two main ways:

    Pods that run a single container. The "one-container-per-Pod" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly.

    Pods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit.

    Grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this pattern only in specific instances in which your containers are tightly coupled.

    You don't need to run multiple containers to provide replication (for resilience or capacity); if you need multiple replicas, see Workload management



The whole point of Kubernetes is to orchestrate the life cycle of a container. We do not interact with particular containers. Instead, the smallest unit we can work with is a Pod. Some would say a pod of whales or peas-in-a-pod. Due to shared resources, the design of a Pod typically follows a one-process-per-container architecture.

Containers in a Pod are started in parallel by default. As a result, there is no way to determine which container becomes available first inside a Pod. initContainers can be used to ensure some containers are ready before others in a pod. To support a single process running in a container, you may need logging, a proxy, or special adapter. These tasks are often handled by other containers in the same Pod.

There is only one IP address per Pod with most network plugins. HPE Labs created a plugin which allows more than one IP per pod. As a result, if there is more than one container, they must share the IP. To communicate with each other, they can use IPC, the loopback interface, or a shared filesystem.

While Pods are often deployed with one application container in each, a common reason to have multiple containers in a Pod is for logging. You may find the term sidecar for a container dedicated to performing a helper task, like handling logs and responding to requests, as the primary application container may not have this ability.

Pods and other objects can be created in several ways. They can be created by using run:

```bash
$ kubectl run newpod --image=nginx
```

Or, they can be created and deleted using properly formatted JSON or YAML files:

```bash
$ kubectl create -f newpod.yaml

$ kubectl delete -f newpod.yaml
```

Other objects will be created by operators/watch-loops to ensure the specifications and current status are the same.

## Single IP per Pod

A Pod represents a group of co-located containers with some associated data volumes. All containers in a Pod share the same network namespace.
To communicate with each other, containers can use the loopback interface, write to files on a common filesystem, or via inter-process communication (IPC). As a result, co-locating applications in the same pod may have issues. There is a network plugin which will allow more than one IP address, but so far, it has only been used within HPE labs.

Support for dual-stack, IPv4 and IPv6 continues to increase with each release. For example, in a recent release kube-proxy iptables supports both stacks simultaneously.

A Pod is a group of co-located containers that share the same IP address. From a networking perspective, a Pod can be seen as a virtual machine of physical hosts. The network needs to assign IP addresses to Pods, and needs to provide traffic routes between all Pods on any nodes.

The three main networking challenges to solve in a container orchestration system are:

    Coupled container-to-container communications (solved by the Pod concept)
    Pod-to-Pod communications
    External-to-Pod communications.

Kubernetes expects the network configuration to enable Pod-to-Pod communications to be available; it will not do it for you.

Pods are assigned an IP address prior to application containers being started. The service object is used to connect Pods within the network using ClusterIP addresses, from outside of the cluster using NodePort addresses, and using a load balancer if configured with a LoadBalancer service.

## Components Involved in Controlling Network Traffic

ClusterIP

A ClusterIP is used for traffic within the cluster. A NodePort first creates a ClusterIP, then associates a port of the node to that new ClusterIP. If you create a LoadBalancer service, it will first create a ClusterIP, then a NodePort, and then make an asynchronous request for an external load balancer. If one is not configured to respond, the EXTERNAL-IP will remain in a pending state for the life of the service.

 Ingress Controller or Service Mesh

An Ingress Controller or a service mesh like Istio can also be used to connect traffic to a Pod.

The diagram below shows a multi-container Pod, two services with one for internal traffic only, and an ingress controller. The sidecar container, acting as a logger, is shown writing out storage, just to show a more complete Pod. The pause container, which is only used to retrieve the namespaces and IP addresses.

Calico Pod

Another possible view of a cluster with multiple Pods and services

## CNI Network Configuration File

To provide container networking, Kubernetes is standardizing on the Container Network Interface (CNI) specification. Kubeadm (the Kubernetes cluster bootstrapping tool) uses CNI as the default network interface mechanism.

CNI is an emerging specification with associated libraries to write plugins that configure container networking and remove allocated resources when the container is deleted. Its aim is to provide a common interface between the various networking solutions and container runtimes. As the CNI specification is language-agnostic, there are many plugins from Amazon ECS, to SR-IOV, to Cloud Foundry, and more.

With CNI, you can write a network configuration file:

{
   "cniVersion": "0.2.0",
   "name": "mynet",
   "type": "bridge",
   "bridge": "cni0",
   "isGateway": true,
   "ipMasq": true,
   "ipam": {
       "type": "host-local",
       "subnet": "10.22.0.0/16",
       "routes": [
           { "dst": "0.0.0.0/0" }
            ]
   }
}

This configuration defines a standard Linux bridge named cni0, which will give out IP addresses in the subnet 10.22.0.0/16. The bridge plugin will configure the network interfaces in the correct namespaces to define the container network properly.

## Pod-to-Pod Communication

While a CNI plugin can be used to configure the network of a pod and provide a single IP per pod, CNI does not help you with pod-to-pod communication across nodes.

The early requirement from Kubernetes was the following:

    All pods can communicate with each other across nodes.
    All nodes can communicate with all pods.
    No Network Address Translation (NAT).

Basically, all IPs involved (nodes and pods) are routable without NAT. This can be achieved at the physical network infrastructure if you have access to it (e.g., GKE). Or, this can be achieved with a software defined overlay with solutions like:

    Flannel
    Calico
    Cilium

Most network plugins now support the use of Network Policies, which act as an internal firewall, limiting ingress and egress traffic.

For more information see the "Cluster Networking" Documentation page or the list of networking add-ons.

Each Pod has it's own IP address.The pods will talk via service.